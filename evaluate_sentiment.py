# -*- coding: utf-8 -*-
import os
import re
import pandas as pd
from sklearn.metrics import precision_recall_fscore_support

# ========= æ¨¡å‹é…ç½®ï¼ˆä¸€æ¬¡å®šä¹‰ï¼Œå«ç›®å½•+å±•ç¤ºåï¼‰=========
MODELS = {
    "base": {"folder": "åŸå§‹åŸºåº§æ¨¡å‹", "display": "åŸºåº§æ¨¡å‹"},
    "s":    {"folder": "Sæ€§æ ¼æ¨¡å‹",   "display": "Sæ¨¡å‹"},
    "n":    {"folder": "Næ€§æ ¼æ¨¡å‹",   "display": "Næ¨¡å‹"},
}

# ========= æ•°æ®é›†é…ç½® =========
datasets = [
    {"name":"imdb_sentiment","file":"imdb_sentiment_results.csv",
     "label_map":{"0":"positive","1":"negative"},
     "allowed_labels":None, "label_col":"label","pred_col":"prediction",
     "base_path":"results/sentiment/imdb"},
    {"name":"mental_sentiment","file":"mental_sentiment_results.csv",
     "label_map":None, "allowed_labels":["normal","depression"],
     "label_col":"label","pred_col":"prediction",
     "base_path":"results/sentiment/mental"},
    {"name":"news_sentiment","file":"news_sentiment_results.csv",
     "label_map":{"0":"bearish","1":"bullish","2":"neutral"},
     "allowed_labels":None, "label_col":"label","pred_col":"prediction",
     "base_path":"results/sentiment/news"},
    {"name":"fiqasa_sentiment","file":"fiqasa_sentiment_results.csv",
     "label_map":None, "allowed_labels":["negative","positive","neutral"],
     "label_col":"answer","pred_col":"prediction",
     "base_path":"results/sentiment/fiqasa"},
    {"name":"imdb_sklearn","file":"imdb_sklearn_sentiment_results.csv",
     "label_map":{"0":"negative","1":"positive"},
     "allowed_labels":None, "label_col":"label","pred_col":"prediction",
     "base_path":"results/sentiment/imdb_sklearn"},
    {"name":"sst2","file":"sst2_sentiment_results.csv",
     "label_map":{"0":"negative","1":"positive"},
     "allowed_labels":None, "label_col":"label","pred_col":"prediction",
     "base_path":"results/sentiment/sst2"},
]

# ========= å·¥å…·å‡½æ•° =========
def clean(text: str) -> str:
    if not isinstance(text, str):
        return ""
    return re.sub(r"[^a-z]", "", text.strip().lower())

def build_allowed(ds) -> list:
    if ds["allowed_labels"]:
        allowed = [clean(x) for x in ds["allowed_labels"]]
    elif ds["label_map"]:
        allowed = [clean(x) for x in ds["label_map"].values()]
    else:
        allowed = []
    return sorted(set(allowed))

def map_true_label_series(ds, s: pd.Series) -> pd.Series:
    if ds["label_map"]:
        s = s.astype(str).map(ds["label_map"])
    return s.astype(str).apply(clean)

def extract_pred_label(text: str, allowed: list) -> str:
    if not isinstance(text, str) or not text.strip():
        return "invalid"
    text_l = text.lower()
    earliest, pos = None, 10**12
    for lbl in allowed:
        m = re.search(rf"\b{re.escape(lbl)}\b", text_l)
        if m and m.start() < pos:
            earliest, pos = lbl, m.start()
    return earliest if earliest is not None else "invalid"

def compute_metrics(y_true, y_pred, class_labels):
    import numpy as np
    acc = float(np.mean([t == p for t, p in zip(y_true, y_pred)])) if len(y_true) > 0 else 0.0
    p_m, r_m, f_m, _ = precision_recall_fscore_support(
        y_true, y_pred, labels=class_labels, average="macro", zero_division=0
    )
    p_w, r_w, f_w, _ = precision_recall_fscore_support(
        y_true, y_pred, labels=class_labels, average="weighted", zero_division=0
    )
    return {
        "accuracy": acc,
        "precision_macro": float(p_m),
        "recall_macro": float(r_m),
        "f1_macro": float(f_m),
        "precision_weighted": float(p_w),
        "recall_weighted": float(r_w),
        "f1_weighted": float(f_w),
        "support": int(len(y_true)),
    }

def pick_pred_path(base_path: str) -> str | None:
    """ä¼˜å…ˆè¯»å–çº æ­£åˆå¹¶åçš„æ–‡ä»¶ï¼›è‹¥ä¸å­˜åœ¨åˆ™è¯»åŸå§‹ç»“æœã€‚"""
    relabeled = base_path.replace(".csv", ".relabeled.csv")
    if os.path.exists(relabeled):
        return relabeled
    return base_path if os.path.exists(base_path) else None

# ========= ä¸»æµç¨‹ =========
rows = []

for ds in datasets:
    print(f"ğŸ” å¤„ç†æ•°æ®é›†ï¼š{ds['name']}")
    allowed = build_allowed(ds)
    if not allowed:
        print(f"  âš ï¸ æ•°æ®é›† {ds['name']} æœªèƒ½è§£æåˆ°åˆæ³•æ ‡ç­¾é›†åˆï¼Œè·³è¿‡ã€‚")
        continue

    for mkey, mconf in MODELS.items():
        mfolder = mconf["folder"]
        path = pick_pred_path(os.path.join(ds["base_path"], mfolder, ds["file"]))
        if not path:
            print(f"  âš ï¸ ç¼ºå°‘æ–‡ä»¶ï¼š{os.path.join(ds['base_path'], mfolder, ds['file'])}")
            continue

        df = pd.read_csv(path)

        # çœŸå®æ ‡ç­¾ -> æ¸…æ´—å¹¶ä»…ä¿ç•™åœ¨ allowed å†…çš„æ ·æœ¬
        y_true_all = map_true_label_series(ds, df[ds["label_col"]])
        mask_keep = y_true_all.isin(allowed)
        kept = df[mask_keep].copy()
        if kept.empty:
            print(f"  âš ï¸ {ds['name']} - {mkey}: æ— å¯è¯„ä¼°æ ·æœ¬ã€‚")
            continue

        # é¢„æµ‹æ–‡æœ¬ -> æŠ½å–åˆ°åˆæ³•æ ‡ç­¾ï¼ˆè‹¥æŠ½ä¸åˆ°ä¸º invalidï¼Œä½†ä¸ä¼šè®¡å…¥ class_labels çš„PRFï¼‰
        kept["__pred_raw"] = kept[ds["pred_col"]].astype(str)
        kept["__pred_label"] = kept["__pred_raw"].apply(lambda x: extract_pred_label(x, allowed))

        y_true = map_true_label_series(ds, kept[ds["label_col"]]).tolist()
        y_pred = kept["__pred_label"].tolist()

        metrics = compute_metrics(y_true, y_pred, class_labels=allowed)

        rows.append({
            "dataset": ds["name"],
            "model": mkey,
            "labels": "|".join(allowed),
            **metrics
        })

# ========= å¯¼å‡ºæ±‡æ€» =========
if rows:
    out_df = pd.DataFrame(rows)

    col_order = [
        "dataset", "model", "labels", "support",
        "accuracy",
        "precision_macro", "recall_macro", "f1_macro",
        "precision_weighted", "recall_weighted", "f1_weighted",
    ]
    out_df = out_df[col_order]

    num_cols = [
        "accuracy",
        "precision_macro", "recall_macro", "f1_macro",
        "precision_weighted", "recall_weighted", "f1_weighted",
    ]
    out_df[num_cols] = out_df[num_cols].round(2)

    csv_path = "metrics_summary.csv"
    out_df.to_csv(csv_path, index=False, encoding="utf-8-sig")

    txt_path = "metrics_summary.txt"
    with open(txt_path, "w", encoding="utf-8") as f:
        for dname in sorted(out_df["dataset"].unique()):
            f.write(f"======== {dname} ========\n")
            sub = out_df[out_df["dataset"] == dname].copy()
            sub["model"] = sub["model"].map(lambda k: MODELS[k]["display"]).fillna(sub["model"])
            f.write(sub.drop(columns=["dataset"]).to_string(index=False, float_format=lambda x: f"{x:.2f}"))
            f.write("\n\n")

    print("\nâœ… æŒ‡æ ‡è®¡ç®—å®Œæˆï¼")
    print(f"  - CSV: {csv_path}")
    print(f"  - TXT: {txt_path}")
else:
    print("âš ï¸ æœªç”Ÿæˆä»»ä½•æŒ‡æ ‡ç»“æœï¼Œè¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„ä¸æ•°æ®ã€‚")
