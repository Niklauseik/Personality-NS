import os
import pandas as pd
import re
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# === æ ¹ç›®å½•ï¼Œç¡®ä¿æ˜¯åœ¨ personality/ ä¸‹è¿è¡Œ ===
base_path = "./results/benchmark"

# === æ¨¡å‹æ–‡ä»¶å¤¹åç§° ===
model_folders = ["Næ€§æ ¼æ¨¡å‹", "Sæ€§æ ¼æ¨¡å‹", "åŸå§‹åŸºåº§æ¨¡å‹"]

# === æ•°æ®é›†æ–‡ä»¶åæ˜ å°„ï¼ˆæ–°å¢ GSM8Kï¼‰ ===
files = {
    "ARC (easy)": "arc_easy_test800_results.csv",
    "BoolQ": "boolq_train800_results.csv",
    "GSM8K": "gsm8k_test800_results.csv"
}

# === æå–å‡½æ•° ===
def extract_upper_letter(text):
    match = re.search(r'\b([A-D])\b', str(text).upper())
    return match.group(1) if match else None

def extract_bool(text):
    if isinstance(text, str):
        text_lower = text.lower()
        if 'true' in text_lower:
            return True
        elif 'false' in text_lower:
            return False
    elif isinstance(text, bool):
        return text
    return None

def compute_metrics(y_true, y_pred):
    accuracy = accuracy_score(y_true, y_pred)
    precision, recall, f1, _ = precision_recall_fscore_support(
        y_true, y_pred, average="macro", zero_division=0
    )
    return {
        "accuracy": round(accuracy, 4),
        "precision": round(precision, 4),
        "recall": round(recall, 4),
        "f1": round(f1, 4)
    }

# === GSM8Kï¼šæ›´å®½æ¾çš„æ•°å€¼å‘½ä¸­è¯„æµ‹ï¼ˆä¸ evaluateä¸€ä¸ª ä¿æŒä¸€è‡´ï¼‰ ===
def extract_numbers(text):
    """æå–æ•°å­—åˆ—è¡¨ï¼šç§»é™¤é€—å·å’Œç¾å…ƒç¬¦å·ï¼ŒåŒ¹é…æ•´æ•°/å°æ•°ï¼ˆä¸ evaluateä¸€ä¸ª ä¿æŒä¸€è‡´ï¼‰"""
    text = str(text).replace(",", "").replace("$", "")
    return [float(n) for n in re.findall(r"\d+\.?\d*", text)]

def gsm8k_accuracy_from_numbers(df):
    """åªè¦æ ‡ç­¾ä¸­çš„ç¬¬ä¸€ä¸ªæ•°å­—å‡ºç°åœ¨é¢„æµ‹æ•°å­—åˆ—è¡¨ä¸­å°±ç®—æ­£ç¡®ï¼›åªç»Ÿè®¡èƒ½è§£æå‡ºæ•°å­—çš„æ ·æœ¬"""
    correct, total = 0, 0
    for _, row in df.iterrows():
        label_nums = extract_numbers(row["label"])
        pred_nums = extract_numbers(row["prediction"])
        if not label_nums or not pred_nums:
            continue
        label = label_nums[0]
        if label in pred_nums:
            correct += 1
        total += 1
    acc = correct / total if total else 0.0
    return round(acc, 4), correct, total

# === æ”¶é›†æ‰€æœ‰ç»“æœ ===
all_results = []

for model_name in model_folders:
    model_path = os.path.join(base_path, model_name)

    for dataset_name, filename in files.items():
        file_path = os.path.join(model_path, filename)
        if not os.path.exists(file_path):
            continue

        df = pd.read_csv(file_path)

        if dataset_name == "ARC (easy)":
            df["label_clean"] = df["label"].apply(extract_upper_letter)
            df["prediction_clean"] = df["prediction"].apply(extract_upper_letter)
            df_valid = df.dropna(subset=["label_clean", "prediction_clean"])
            metrics = compute_metrics(df_valid["label_clean"], df_valid["prediction_clean"])

        elif dataset_name == "BoolQ":
            df["label_clean"] = df["label"].apply(extract_bool)
            df["prediction_clean"] = df["prediction"].apply(extract_bool)
            df_valid = df.dropna(subset=["label_clean", "prediction_clean"])
            metrics = compute_metrics(df_valid["label_clean"], df_valid["prediction_clean"])

        elif dataset_name == "GSM8K":
            # ä½¿ç”¨æ›´å®½æ¾çš„æ•°å€¼å‘½ä¸­è¯„æµ‹
            accuracy, correct, total = gsm8k_accuracy_from_numbers(df)
            metrics = {
                "accuracy": accuracy,
                "precision": None,
                "recall": None,
                "f1": None,
                # å¯é€‰ï¼šä¹ŸæŠŠå¯è§£ææ ·æœ¬ç»Ÿè®¡åˆ°è¡¨é‡Œï¼Œä¾¿äºå®¡è®¡ï¼ˆä¸æƒ³å±•ç¤ºå¯ä»¥å»æ‰è¿™ä¸¤åˆ—ï¼‰
                #"parsed_correct": correct,
                #"parsed_total": total,
            }

        all_results.append({
            "Model": model_name,
            "Dataset": dataset_name,
            **metrics
        })

# === è¾“å‡ºä¸º DataFrame ç»“æœè¡¨ ===
df_metrics = pd.DataFrame(all_results)
print(df_metrics)

# === ä¿å­˜ç»“æœåˆ° txt æ–‡ä»¶ ===
output_path = os.path.join(base_path, "benchmark_metrics_summary.txt")

with open(output_path, "w", encoding="utf-8") as f:
    for _, row in df_metrics.iterrows():
        f.write(
            f"\nğŸ“Œ Model: {row['Model']}\n"
            f"ğŸ“Š Dataset: {row['Dataset']}\n"
            f"âœ… Accuracy: {row['accuracy']}\n"
            f"âœ… Precision: {row['precision']}\n"
            f"âœ… Recall: {row['recall']}\n"
            f"âœ… F1 Score: {row['f1']}\n"
            f"{'-'*40}\n"
        )

print(f"\nğŸ“ å·²å°†ç»“æœä¿å­˜åˆ°ï¼š{output_path}")
